# -*- coding: utf-8 -*-
"""GCN_TDA_version_finale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zIQtkvHrkfY8fEQNQgQi-hLn3uynjOsp
"""

! pip install tensorflow

!pip install torch-geometric

! pip install giotto-tda

import numpy as np # algèbre linéaire
import pandas as pd # data processing
import matplotlib.pyplot as plt
import seaborn as sns

#Importing libraries.
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import PolynomialFeatures, StandardScaler

# Sklearn
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

import pandas as pd

dataset = pd.read_excel('/content/Base de donnees greffe pulmonaire_Ian.xlsx')

dataset = dataset.drop(['Unnamed: 0', 'NIP (identifiant)', 'date de TP','mortalité à 90j','mortalité à J30'], axis = 1)

dataset.columns

dataset.isna().sum(axis = 0)

dataset = dataset.fillna(dataset.mean())

X= dataset.drop(labels = "mortalité à un an", axis = 1)
y= dataset["mortalité à un an"]

# This scales each column to have mean=0 and standard deviation=1
SS= StandardScaler()
X_SS = SS.fit_transform(X)
# Apply scaling
df=pd.DataFrame(X_SS, columns=X.columns)

#df.head(10)
corr = dataset.corr()

import networkx as nx
import numpy as np
import argparse
import torch
import torch.nn as nn
from torch.nn import Module

import torch.nn.functional as F
import torch.optim as optim

import matplotlib.pyplot as plt
import torchvision
from torchvision import datasets, models, transforms

patient_dict = {"Patient "+str(index): row.values for index, row in df.iterrows()}

# Sample data (replace this with your actual data)
da_ta =patient_dict

# Create a DataFrame from the data
df1 = pd.DataFrame(da_ta)

# Calculate the correlation matrix
correlation_matrix = df1.corr()

# Create a network graph
G = nx.Graph()

# Add nodes (variables) to the graph
for column in df1.columns:
   G.add_node(column)

# Add edges (connections) between nodes based on correlation
for i, node1 in enumerate(df1.columns):
   for j, node2 in enumerate(df1.columns):
       if i < j :  # To avoid adding duplicate edges
           correlation = correlation_matrix.iloc[i, j]
           if abs(correlation) > 0.5:  # Set a threshold for correlation strength
               G.add_edge(node1, node2, weight=correlation)

# Draw the network
pos = nx.spring_layout(G, seed=42)  # Layout the nodes using the spring layout algorithm
nx.draw_networkx(G, pos, with_labels=True, font_size=4, node_color='skyblue', node_size=500,
       edge_color=[d['weight'] for u, v, d in G.edges(data=True)], cmap=plt.cm.coolwarm)

# Add edge weights as labels
edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)

# Show the plot
plt.title("Correlation Network")
plt.axis('off')
plt.show()

import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

# Création un DataFrame à partir des données (remplacez da_ta par vos données)
df1 = pd.DataFrame(da_ta)

# Calcul de la matrice de corrélation
correlation_matrix = df1.corr()

# Créer le graphe
G = nx.Graph()

# Ajout des nœuds (variables) au graphe
for column in df1.columns:
    G.add_node(column)

# Ajout des arêtes (connexions) entre les nœuds en fonction de la corrélation
for i, node1 in enumerate(df1.columns):
    for j, node2 in enumerate(df1.columns):
        if i < j:  # Pour éviter d'ajouter des arêtes en double
            correlation = correlation_matrix.iloc[i, j]
            if abs(correlation) > 0.6:  # Définir un seuil pour la force de corrélation
                G.add_edge(node1, node2, weight=correlation)

# Dessiner le réseau
pos = nx.circular_layout(G)  # Disposer les nœuds en utilisant l'algorithme de disposition spring
node_colors = 'skyblue'
node_size = 50
edge_colors = [d['weight'] for u, v, d in G.edges(data=True)]
edge_labels = {(u, v): f"{d['weight']:.2f}" for u, v, d in G.edges(data=True)}

plt.figure(figsize=(10, 10))
nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_size)
nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=plt.cm.coolwarm)
nx.draw_networkx_labels(G, pos, font_size=4)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=4)
plt.title("Correlation Network")
plt.axis('off')
plt.show()

pos = nx.spring_layout(G, seed=42)  # Disposer les nœuds en utilisant l'algorithme de disposition spring

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

# Calcul de la matrice de corrélation
correl = df1.corr(method='pearson')

# Discrétisation des valeurs de corrélation en 6 catégories
num_bins = 6
min_corr = np.min(correl.values)
max_corr = np.max(correl.values)
labels = np.digitize(correl.values.flatten(), bins=np.linspace(min_corr, max_corr, num_bins + 1))

# Convertir les labels en couleurs
color_map = plt.get_cmap('viridis', num_bins)  # Choisissez une colormap appropriée
node_color = [color_map(label) for label in labels]

# Créer le graphe
G = nx.Graph()

# Ajout des nœuds pour chaque variable
for column in df1.columns:
    G.add_node(column, size=20)

# Ajout des arêtes entre les nœuds
for col1 in df1.columns:
    for col2 in df1.columns:
        if col1 != col2:
            correlation = correl.loc[col1, col2]
            if abs(correlation) > 0.5:  # Définir un seuil pour la force de corrélation
                G.add_edge(col1, col2, weight=correlation)

# Dessiner le graphe
pos = nx.circular_layout(G)  # Ajustez le layout comme nécessaire

plt.figure(figsize=(10, 10))
nx.draw_networkx_nodes(G, pos, node_size=8)
nx.draw_networkx_edges(G, pos, alpha=0.2)
nx.draw_networkx_labels(G, pos, font_size=6)
plt.title("Correlation Network")
plt.axis('off')
plt.show()

# Affichage quelques informations sur le graphe
print(f"Nombre de nœuds : {G.number_of_nodes()}")
print(f"Nombre d'arêtes : {G.number_of_edges()}")

import networkx as nx
import torch
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv
from sklearn.model_selection import train_test_split

edges = list(G.edges())
# Création d'un dictionnaire pour mapper les noms de nœuds aux identifiants numériques
node_to_id = {}
id_counter = 0
for edge in edges:
    for node in edge:
        if node not in node_to_id:
            node_to_id[node] = id_counter
            id_counter += 1

# Conversion des noms de nœuds en identifiants numériques dans la liste d'arêtes
edges_numeric = [(node_to_id[edge[0]], node_to_id[edge[1]]) for edge in edges]

# Conversion de la liste d'arêtes en un tenseur edge_index
edge_index = torch.tensor(edges_numeric, dtype=torch.long).t().contiguous()

print(edge_index)



# Compter les occurrences de chaque classe
class_counts = dataset["mortalité à un an"].value_counts()

# Convertir en tenseur
class_counts_tensor = torch.tensor(class_counts.values)
num_classes = len(class_counts_tensor)

# Affichage du nombre de classes et des occurrences de chaque classe
print("Nombre de classes :", len(class_counts_tensor))
print("Occurrences de chaque classe :\n", class_counts)
print("Occurrences de chaque classe (tenseur) :\n", class_counts_tensor)

# Conversion le graphe NetworkX en format PyG Data
x = torch.eye(G.number_of_nodes())  # Matrice d'identité comme caractéristiques initiales

# Conversion les étiquettes en tenseurs PyTorch
labels = torch.tensor(y, dtype=torch.long)


# y = data["mortalité à un an"]

# Diviser les données en ensembles d'entraînement et de test
train_indices, test_indices = train_test_split(list(range(G.number_of_nodes())), test_size=0.2, random_state=42)
# Division des données en ensembles d'entraînement, de test et de validation

train_indices, val_indices = train_test_split(train_indices, test_size=0.1, random_state=42)

# Conversion des indices en tenseurs PyTorch
train_mask = torch.tensor(train_indices)
test_mask = torch.tensor(test_indices)
val_mask = torch.tensor(val_indices)

data = Data(edge_index=edge_index, x=x, y = labels, num_classes = class_counts_tensor, train_mask=train_mask, test_mask=test_mask, val_mask = val_mask)



print(f'x = {data.x.shape}')
print(data.x)

print(f'edge_index = {data.edge_index.shape}')
print(data.edge_index)

from torch_geometric.utils import to_dense_adj

A = to_dense_adj(data.edge_index)[0].numpy().astype(int)
print(f'A = {A.shape}')
print(A)

print(f'labels = {data.y.shape}')
print(data.y)

print(f'train_mask = {data.train_mask.shape}')
print(data.train_mask)

print(f'Edges are directed: {data.is_undirected()}')
print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')
print(f'Graph has loops: {data.has_self_loops()}')

from torch_geometric.utils import to_networkx

G = to_networkx(data, to_undirected=True)
plt.figure(figsize=(10,10))
plt.axis('off')
nx.draw_networkx(G,
                pos=nx.circular_layout(G),
                with_labels=True,
                node_size=800,
                node_color=data.y,
                cmap="hsv",
                vmin=-2,
                vmax=2,
                width=0.8,
                edge_color="grey",
                font_size=4
                )
plt.show()

from torch_geometric.utils import to_networkx

G = to_networkx(data, to_undirected=True)
plt.figure(figsize=(10,10))
plt.axis('off')
nx.draw_networkx(G,
                pos=nx.spring_layout(G, seed=0),
                with_labels=True,
                node_size=800,
                node_color=data.y,
                cmap="hsv",
                vmin=-2,
                vmax=3,
                width=0.8,
                edge_color="grey",
                font_size=14
                )
plt.show()

import networkx as nx
# Calculer les mesures topologiques
degree_centrality = nx.degree_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)

# Imprimer les mesures topologiques pour chaque patient
for patient, degree in degree_centrality.items():
    closeness = closeness_centrality[patient]
    betweenness = betweenness_centrality[patient]
    print(f"patient: {patient} | Degré de centralité: {degree} | Centralité de proximité: {closeness} | Centralité d'intermédiarité: {betweenness}")

# Extraction des valeurs de centralité de degré
degree_values = list(degree_centrality.values())
degree_centrality_tensor = torch.tensor(degree_values, dtype=torch.float32).unsqueeze(1)  # Ajout d'une dimension

# Extraction des valeurs de centralité de proximité
closeness_values = list(closeness_centrality.values())
closeness_centrality_tensor = torch.tensor(closeness_values, dtype=torch.float32).unsqueeze(1)  # Ajout d'une dimension

# Extraction des valeurs de centralité d'interdépendance
betweenness_values = list(betweenness_centrality.values())
betweenness_centrality_tensor = torch.tensor(betweenness_values, dtype=torch.float32).unsqueeze(1)  # Ajout d'une dimension

# Concaténation des tenseurs le long de dim=1
combined_features = torch.cat((data.x, degree_centrality_tensor, closeness_centrality_tensor, betweenness_centrality_tensor), dim=1).unsqueeze(1)

from torch.nn import Linear
from torch_geometric.nn import GCNConv

# Créez votre modèle GCN
class GCN(torch.nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.gcn = GCNConv(input_dim, 64)  # Utilisez 64 comme dimension cachée arbitraire
        self.out = Linear(64, num_classes)

    def forward(self, x, edge_index):
        h = self.gcn(x, edge_index).relu()
        z = self.out(h)
        return h, z

# Définir le modèle

input_dim = combined_features.shape[1]  # Nombre de caractéristiques d'entrée
num_classes = 2  # nombre de classes approprié
model = GCN(input_dim, num_classes)

# Définissez la fonction de perte et l'optimiseur
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.02)

# Calculate accuracy
def accuracy(pred_y, y):
    return (pred_y == y).sum() / len(y)

    # Data for animations
embeddings = []
losses = []
accuracies = []
outputs = []

# Boucle d'entraînement
for epoch in range(201):

    optimizer.zero_grad()


    # Les caractéristiques topologiques aux caractéristiques d'entrée
    combined_features = torch.cat((data.x, degree_centrality_tensor, closeness_centrality_tensor, betweenness_centrality_tensor), dim=1)

    h, z = model(combined_features, data.edge_index)

    # Calcul de la fonction de perte
    loss = criterion(z, data.y)

    # Calcul de l'exactitude
    acc = accuracy(z.argmax(dim=1), data.y)

    # Calcul des gradients et mise à jour des paramètres
    loss.backward()
    optimizer.step()

    # Stockage des données pour l'animation
    embeddings.append(h)
    losses.append(loss)
    accuracies.append(acc)
    outputs.append(z.argmax(dim=1))

    # Affichage des métriques toutes les 10 époques
    if epoch % 10 == 0:
        print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')



# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from IPython.display import HTML
# from matplotlib import animation
# plt.rcParams["animation.bitrate"] = 3000
# 
# def animate(i):
#     G = to_networkx(data, to_undirected=True)
#     nx.draw_networkx(G,
#                     pos=nx.spring_layout(G,seed = 0),
#                     with_labels=True,
#                     node_size=500,
#                     node_color=outputs[i],
#                     cmap="hsv",
#                     vmin=-2,
#                     vmax=3,
#                     width=0.8,
#                     edge_color="grey",
#                     font_size=14
#                     )
#     plt.title(f'Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%',
#               fontsize=18, pad=20)
# 
# fig = plt.figure(figsize=(10, 10))
# plt.axis('off')
# 
# anim = animation.FuncAnimation(fig, animate, \
#             np.arange(0, 200, 10), interval=500, repeat=True)
# html = HTML(anim.to_html5_video())
# 
#

display(html)

# Print embeddings
print(f'Final embeddings = {h.shape}')
print(h)

# Get first embedding at epoch = 0
embed = h.detach().cpu().numpy()

fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(projection='3d')
ax.patch.set_alpha(0)
plt.tick_params(left=False,
                bottom=False,
                labelleft=False,
                labelbottom=False)
ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2],
           s=200, c=data.y, cmap="hsv", vmin=-2, vmax=3)

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# def animate(i):
#     embed = embeddings[i].detach().cpu().numpy()
#     ax.clear()
#     ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2],
#            s=200, c=data.y, cmap="hsv", vmin=-2, vmax=3)
#     plt.title(f'Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%',
#               fontsize=18, pad=40)
# 
# fig = plt.figure(figsize=(8,8))
# plt.axis('off')
# ax = fig.add_subplot(projection='3d')
# plt.tick_params(left=False,
#                 bottom=False,
#                 labelleft=False,
#                 labelbottom=False)
# 
# anim = animation.FuncAnimation(fig, animate, \
#               np.arange(0, 200, 10), interval=800, repeat=True)
# html = HTML(anim.to_html5_video())

display(html)

"""# Topological Analysis Data"""

import numpy as np
from gtda.homology import VietorisRipsPersistence
from gtda.diagrams import Amplitude

y = dataset['mortalité à un an']
X_train, X_test, y_train, y_test = train_test_split(X_SS, y, test_size=0.2, random_state=42)

#converting fromnumpy to torch tensor
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)
# Conversion de y_train_array et y_test_array en tenseurs PyTorch
y_train = torch.LongTensor(y_train)
#y_test = torch.LongTensor(y_test)

persistence = VietorisRipsPersistence(metric = 'euclidean',homology_dimensions=[0,1,2],n_jobs=-1,collapse_edges=True)

X_pers =persistence.fit_transform_plot(X_train[None,:,:])

#"constructing our model"
class Model(nn.Module):
    def __init__(self,input_dim,output_dim):
        super(Model,self).__init__()
        self.input_layer    = nn.Linear(input_dim,128)
        self.hidden_layer1  = nn.Linear(128,64)
        self.output_layer   = nn.Linear(64,output_dim)
        self.relu = nn.ReLU()


    def forward(self,x):
        out =  self.relu(self.input_layer(x))
        out =  self.relu(self.hidden_layer1(out))
        out =  self.output_layer(out)
        return out

def get_accuracy_multiclass(pred_arr,original_arr):
    if len(pred_arr)!=len(original_arr):
        return False
    pred_arr = pred_arr.numpy()
    original_arr = original_arr.numpy()
    final_pred= []
    for i in range(len(pred_arr)):
        final_pred.append(np.argmax(pred_arr[i]))
    final_pred = np.array(final_pred)
    count = 0
    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy
    for i in range(len(original_arr)):
        if final_pred[i] == original_arr[i]:
            count+=1
    return count/len(final_pred)

persistence = VietorisRipsPersistence(metric = 'euclidean',homology_dimensions=[0,1,2],n_jobs=-1,collapse_edges=True)
def eachEpoch(epoch, train_losses,test_losses):
    optimizer.zero_grad()

    #forward feed
    output_train = model(X_train)



    #transforming my input dataset into a 3d array and calcualting it persistence entropy
    X_pers=persistence.fit_transform(X_train[None,:,:])
    #calculating the amplitude
    X_Am_1 = Amplitude(metric='bottleneck').fit_transform(X_pers)
    X_Am_2 = Amplitude(metric='wasserstein').fit_transform(X_pers)
    X_Am_3 = Amplitude(metric='landscape').fit_transform(X_pers)
    X_Am_4 = Amplitude(metric='betti').fit_transform(X_pers)
    X_Am_5 = Amplitude(metric='persistence_image').fit_transform(X_pers)

    ampl_X = X_Am_1 + X_Am_2 + X_Am_3 + X_Am_4 + X_Am_5

    #same for output y
    y_pers =persistence.fit_transform(output_train.detach().numpy()[None,:,:])
    y_Am_1=Amplitude(metric='bottleneck').fit_transform(y_pers)
    y_Am_2=Amplitude(metric='wasserstein').fit_transform(y_pers)
    y_Am_3=Amplitude(metric='landscape').fit_transform(y_pers)
    y_Am_4=Amplitude(metric='betti').fit_transform(y_pers)
    y_Am_5=Amplitude(metric='persistence_image').fit_transform(y_pers)

    ampl_y = y_Am_1 + y_Am_2 + y_Am_3 + y_Am_4 + y_Am_5
    l=(np.linalg.norm(ampl_X-ampl_y)**2)/2


    #calculate the loss
    loss_train = criterion(output_train, y_train)+(1e-6)*l

    #backward propagation: calculate gradients
    loss_train.backward()

    #update the weights
    optimizer.step()


    output_test = model(X_test)
    loss_test = criterion(output_test,y_test)

    train_losses[epoch] = loss_train.item()
    test_losses[epoch] = loss_test.item()

    #print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}")
    print(f"Epoch {epoch}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}")

    return [epoch + 1,loss_test.item()]



# input_dim = 40 because we have 40 inputs
# output_dim = 2 because we have namely 2 categories dead or alive
input_dim  = 41
output_dim = 2
model = Model(input_dim,output_dim)
model

# creating our optimizer and loss function object
learning_rate = 0.0001
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)

#utils.validate_py_environment(environment, episodes=6)
num_epochs = 1000
train_losses = np.zeros(num_epochs)
test_losses  = np.zeros(num_epochs)

### Calculating test accuracy
def testAccuracy(X_test,epoch):
    epoch = eachEpoch(epoch)
    predictions_test =  []
    with torch.no_grad():
        predictions_test = model(X_test)
    test_acc  = get_accuracy_multiclass(predictions_test,y_test)

    print(test_acc)
    return [round(test_acc*100,3),epoch]



num_epochs = 201
for epoch in range(num_epochs):
    # Appelez la fonction eachEpoch() avec les arguments nécessaires
    epoch, test_loss = eachEpoch(epoch, train_losses, test_losses)



plt.figure(figsize=(10,10))
plt.plot(train_losses, label='train loss')
plt.plot(test_losses, label='test loss')
plt.legend()
plt.show()

predictions_train = []
predictions_test =  []
with torch.no_grad():
    predictions_train = model(X_train)
    predictions_test = model(X_test)

train_acc = get_accuracy_multiclass(predictions_train,y_train)
test_acc  = get_accuracy_multiclass(predictions_test,y_test)

print(f"Training Accuracy: {round(train_acc*100,3)}")
print(f"Test Accuracy: {round(test_acc*100,3)}")